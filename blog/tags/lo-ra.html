<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">One post tagged with &quot;LoRA&quot; | sd99</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://sam99dave.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://sam99dave.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://sam99dave.github.io/blog/tags/lo-ra"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="One post tagged with &quot;LoRA&quot; | sd99"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/new_logo.png"><link data-rh="true" rel="canonical" href="https://sam99dave.github.io/blog/tags/lo-ra"><link data-rh="true" rel="alternate" href="https://sam99dave.github.io/blog/tags/lo-ra" hreflang="en"><link data-rh="true" rel="alternate" href="https://sam99dave.github.io/blog/tags/lo-ra" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="sd99 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="sd99 Atom Feed"><link rel="stylesheet" href="/assets/css/styles.5c8c3f00.css">
<link rel="preload" href="/assets/js/runtime~main.782de680.js" as="script">
<link rel="preload" href="/assets/js/main.54f950d1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/new_logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/new_logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">sd99</b></a><a class="navbar__item navbar__link" href="/docs/intro">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sam99dave" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/LoRA ~ Low-Rank Adaptation">LoRA ~ Low-Rank Adaptation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/welcome">Welcome</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/mdx-blog-post">MDX Blog Post</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/long-blog-post">Long Blog Post</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/first-blog-post">First Blog Post</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;LoRA&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/LoRA ~ Low-Rank Adaptation">LoRA ~ Low-Rank Adaptation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-09-23T15:10:05.552Z" itemprop="datePublished">September 23, 2023</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/wgao19" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/wgao19.png" alt="Samuel Davis"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/wgao19" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Samuel Davis</span></a></div><small class="avatar__subtitle" itemprop="description">Docusaurus Core Team</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Fine Tuning models such as GPT-3 ( 175B ) from scratch requires lot of compute which is very costly. <strong>Lo</strong>w <strong>R</strong>ank <strong>A</strong>daptation ( <strong>LoRA</strong> ) freezes the pretrained model weights &amp; injects trainable rank decomposition matrices. These matrices are injected into each layer of the Transformer architecture. This reduces the number of trainable parameters for downstream tasks.</p><blockquote><p>Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times.</p></blockquote><p>LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during
adaptation instead, while keeping the pre-trained weights frozen</p><p><strong>Advantages</strong></p><p>→ A pre-trained model can be shared to build LoRA modules for different tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices <em>A</em> and <em>B,</em> reducing the storage requirement and task-switching overhead significantly.</p><p>→ LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.</p><p>→ The simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, <em>introducing no inference latency</em> compared to a fully fine-tuned model, by construction.</p><p>→ LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning.</p><p>Read From <a href="https://www.ml6.eu/blogpost/low-rank-adaptation-a-technical-deep-dive#:~:text=The%20main%20idea%20behind%20LoRA,a%20low%2Drank%20matrix%20i.e." target="_blank" rel="noopener noreferrer">here</a> and explain details of LoRA</p><hr><p>During full fine-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ by repeatedly following the gradient to maximize the conditional language modeling objective:</p><p><img loading="lazy" alt="Untitled" src="/assets/images/fine tuning-63769c81ffd6d5e3ca6b6b5e8bfe49e1.png" width="388" height="92" class="img_ev3q"></p><p>One of the main drawbacks for full fine-tuning is that for <em>each</em> downstream task, we learn a <em>different</em> set of parameters ∆Φ whose dimension <em>|</em>∆Φ<em>|</em> equals <em>|</em>Φ0<em>|</em>.</p><p>If a model has weight matrices of size WxH then the delta will also be of the same dimension. Therefore, while finetuning if there is a model with dimension 175B then the delta will also require a storage space to save these gradients of size 175B before updating the weights.</p><p>Moreover, storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible.</p><p>In this paper, we adopt a more parameter-efficient approach, where the task-specific parameter increment ∆Φ = ∆Φ(Θ) is further <strong>encoded by a much smaller-sized set of parameters</strong> Θ with <em>|</em>Θ<em>| &lt;&lt; |</em>Φ0<em>|</em>. </p><p>The task of finding ∆Φ thus becomes optimizing over Θ:</p><p><img loading="lazy" alt="Untitled" src="/assets/images/updated-eq-92fdb8e2cbc5898d1f344279b09e0ca9.png" width="463" height="98" class="img_ev3q"></p><p>The authors propose to use a low-rank representation to encode ∆Φ that is both <strong>compute- and memory-efficient</strong>. </p><p>When the pre-trained model is GPT-3 175B, the number of train able parameters <em>|</em>Θ<em>|</em> can be as small as 0<em>.</em>01% of <em>|</em>Φ0<em>|</em> ( 175B → 17.5M ).</p><p>Aren’t Existing Solutions Good Enough?</p><p>This issue has been there since <em>Transfer Learning</em> </p><p>Many techniques have been developed trying to tackle this. From Language Modelling POV, there are two strategies:</p><p>→ Adding Adapter layers</p><p>→ Optimizing some forms of the input layer activation</p><p>However, there are limitations to them</p><p><strong>Adding Adapter layers</strong> → Introduces Inference Latency</p><p>→ One can think of reducing the latency by either pruning the layers or by exploiting multi-task settings but there is no direct way to bypass the extra compute that the adapters add in. It might seem that its fine as the adapters have few parameters (almost &lt; 1% of the model) due to bottleneck dimensions, which limits the FLOPs they can add. Large Neural Networks rely on GPU parallelism to keep the latency low, <strong>adapter layers have to be processed sequentially</strong>. On performing inference using GPT-2 medium on a single GPU (scenario without model parallelism),</p><p>we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension</p><p><img loading="lazy" alt="Untitled" src="/assets/images/adapter L-H issues-05e833c3e52a0267f59fc81206ce2691.png" width="1161" height="438" class="img_ev3q"></p><hr><p><strong>LoRA</strong></p><p>→ LOW-RANK-PARAMETRIZED UPDATE MATRICES</p><p>A neural network contains many dense layers which perform matrix multiplication. The weight
matrices in these layers typically have <strong>full-rank.</strong></p><p><strong>Lets check out Ranks first</strong></p><p>Matrix Rank</p><p>The <strong>rank of a matrix</strong> is the dimension of the vector space generated by its columns, which is given by the <strong>number of linearly independent columns (or rows) in a given matrix.</strong> </p><p>It can be proven that the number of independent columns (known as <em>column rank)</em> is always equal to the number of independent rows (called <em>row rank)</em>. Hence, for a matrix <strong><em>A</em></strong> with <strong><em>m</em></strong> rows and <strong><em>n</em></strong> columns (represented as <strong>*Aₘₙ</strong>)*,</p><p><img loading="lazy" alt="Untitled" src="/assets/images/rank matrix-1c69f56f2fe82c7c11ecfe598fce83fa.png" width="607" height="112" class="img_ev3q"></p><p><strong>Linear Dependence &amp; Independence</strong></p><p>In the theory of <a href="https://en.wikipedia.org/wiki/Vector_space" target="_blank" rel="noopener noreferrer">vector spaces</a>, a <a href="https://en.wikipedia.org/wiki/Set_(mathematics)" target="_blank" rel="noopener noreferrer">set</a> of <a href="https://en.wikipedia.org/wiki/Vector_(mathematics)" target="_blank" rel="noopener noreferrer">vectors</a> is said to be <strong>linearly independent</strong> if there exists no nontrivial <a href="https://en.wikipedia.org/wiki/Linear_combination" target="_blank" rel="noopener noreferrer">linear combination</a> of the vectors that equals the zero vector.</p><p>→ Linear Dependent</p><p><img loading="lazy" alt="Untitled" src="/assets/images/Linear Dependent vectors-d64cfe228dee311869daa438b93d4227.png" width="913" height="196" class="img_ev3q"></p><p>→ Linear Independent</p><p><img loading="lazy" alt="Untitled" src="/assets/images/Linear Independent Vectors-a197ba7ebeed82b34eb1b2508b990a24.png" width="1001" height="73" class="img_ev3q"></p><p>Can only be satisfied if a(i) = 0 for i = 1,…,n</p><p>Based on these Ranks matrices can be classified into 2 types:</p><p>→ Full Rank</p><p>A matrix <strong><em>Aₘₙ</em></strong> is called a <strong>full-rank matrix</strong> if <strong><em>rank(A) =</em> <em>min(m, n)</em>.</strong> The matrix shown below is an example of a full rank matrix.</p><p><img loading="lazy" alt="Untitled" src="/assets/images/full rank-27ed691f14c1b0c85412a9cb348e6698.png" width="672" height="257" class="img_ev3q"></p><p>→ Rank Deficient</p><p>The opposite of a full rank matrix is <strong>rank deficient</strong> i.e. <strong><em>rank(A)</em> &lt; <em>min(m, n)</em></strong>. The rank-deficient matrix shown below has a rank of <strong><em>1</em></strong>, as the columns (or rows) of the matrix are not linearly independent of one another.</p><p><img loading="lazy" alt="Untitled" src="/assets/images/rank deficient-15cc679aab22ccd638350c6bdfe0f776.png" width="651" height="177" class="img_ev3q"></p><p><em>L<strong>ow-Rank Matrix</strong>: A rank-deficient matrix <strong>Aₘₙ</strong> is called a low-rank matrix if its rank is significantly lower (no fixed threshold) than the minimum number of rows and columns. Mathematically, <strong>rank(A) &lt;&lt; min(m, n)</strong>.</em></p><p>Rank Decomposition</p><p>Rank decomposition or factorization of a matrix <strong><em>Aₘₙ</em></strong> is the factorization of <strong><em>A</em></strong> of the form <strong><em>A</em></strong> <strong><em>= CₘᵣFᵣₙ</em></strong> where <strong><em>rank(A) =</em> <em>r</em></strong>. It can be proven that every (finite) matrix has a rank decomposition. </p><p>Techniques like SVD (Singular Value Decomposition) can be used to construct such a decomposition.</p><p>When adapting to a task the pretrained model weights have “low intrinsic dimensions”. When adapting to a specific task (fine-tuning) we just randomly project it to a smaller subspace still it learns efficiently. Hence it can be hypothesized that the change in weights (update) also has a “low intrinsic dimensions/rank”. </p><p>Therefore, we can use rank decomposition for the change in weights.</p><p><img loading="lazy" alt="Untitled" src="/assets/images/lora-rankdecomposition-c298c722f64ada572e6ff87c1c35a53e.png" width="1131" height="212" class="img_ev3q"></p><p>E.g.</p><p>If d = 100 &amp; k = 100</p><p>then d * k = 10000</p><p>Now, if r ~ 5</p><p>then ( d <em> r ) + ( r </em> k ) = ( 500 ) + ( 500 ) = 1000</p><p>Its clear that how this reduces the storage requirements.</p><p>Initial Initialization,</p><p>A → Random Gaussian initialization</p><p>B → 0</p><p>Therefore, initially AB → 0. Moreover, it also scaled by (<em>alpha / rank</em>)</p><p>Where <em>alpha</em> is a constant (this is set to the first selected rank). This helps to reduce the need to retune hyperparameters when varying <em>rank</em></p><p>Latency</p><p><img loading="lazy" alt="Untitled" src="/assets/images/Latency-be5fccd449900d3bc00aae36854c0c82.png" width="1135" height="141" class="img_ev3q"></p><p>Instead of saving 2 entire different models we use a shared pretrained model with task related adapters.</p><p><strong>Practical Benefits &amp; Limitations</strong></p><p>→ The most significant benefit comes from the reduction in memory and storage usage as we do not need to store the optimizer states for the frozen parameters. This saves a lot of VRAM.</p><p>→ The size of the checkpoint also significantly reduces. </p><p>→ Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters.</p><p>For GPT 3 175B, a speedup of 25% was observed for training compared to full fine-tuning.</p><p>Limitations</p><p>→ It is not straightforward to batch inputs to different tasks with different <em>A</em> and <em>B</em> in a single forward pass.</p><hr><p>Storage Reduction</p><p><img loading="lazy" alt="Untitled" src="/assets/images/storage reduction-ef9f31d51f8c68962bba39a95cdb096c.png" width="1116" height="132" class="img_ev3q"></p><p><strong>UNDERSTANDING THE LOW-RANK UPDATES</strong></p><p>WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?</p><p><img loading="lazy" alt="Untitled" src="/assets/images/identifying weights to use LoRA-8292733a8ae00ca618cc2ee28232b5e3.png" width="1081" height="222" class="img_ev3q"></p><p>This is for GPT 3 175B, the budget for trainable parameters is set to 18M which ~ 35MB in FP16 storage.</p><p>The above figure shows us that:</p><p>→ Wq, Wv ~ Gives the best performance.</p><p>The figure suggests that even a rank of four captures enough information in ∆<em>W</em> such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.</p><p>WHAT IS THE OPTIMAL RANK <em>r</em> FOR LORA?</p><p><img loading="lazy" alt="Untitled" src="/assets/images/optimal rank-dfb758f654baa4c39e00e8a65ced06cb.png" width="993" height="278" class="img_ev3q"></p><p>LoRA already performs competitively with a very small <em>r</em> (more so for <em>{Wq, Wv}</em> than just <em>Wq</em>)</p><p>This suggests the update matrix ∆<em>W</em> could have a very small <strong>intrinsic rank</strong></p><p>To further support this finding, the authors checked the overlap of the subspaces learned by different choices of <em>r</em> and by different random seeds.</p><p>The authors argue that increasing <em>r</em> does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.</p><p>HOW DOES THE ADAPTATION MATRIX ∆<em>W</em> COMPARE TO <em>W</em> ?</p><p><img loading="lazy" alt="Untitled" src="/assets/images/delta W vs W - part 1-7e5f3cb2d777cd106324a96701166e75.png" width="1143" height="502" class="img_ev3q"></p><p>Note: There are detailed figures regarding these studies in the Appendix section of the paper</p><p><img loading="lazy" alt="Untitled" src="/assets/images/delta W and W - part 2-37a9a50b7d9ea6b0a50a1debdc9b0608.png" width="1127" height="302" class="img_ev3q"></p><p>Conclusion from the above figure:</p><p>→ ∆<em>W</em> has a stronger correlation with <em>W</em> compared to a random matrix, indicating that ∆<em>W</em> amplifies some features that are already in <em>W</em>.</p><p>→ ∆<em>W ~ 0.32 | W ~ 21.67</em></p><p>→ Random ~ 0.02 | <em>W ~ 21.67</em></p><p>→ Instead of repeating the top singular directions of <em>W</em>, ∆<em>W</em> only <em>amplifies directions that are not emphasized in W.</em></p><p>→ The amplification factor is rather huge: </p><p>→ 21<em>.</em>5 <em>≈</em> 6<em>.</em>91<em>/</em>0<em>.</em>32 for <em>r</em> = 4 when compared to <em>r</em> = 64</p><p>This suggests that the low-rank adaptation matrix potentially <strong><em>amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model</em></strong>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/lo-ra">LoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/large-language-models">Large Language Models</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/sam99dave" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.782de680.js"></script>
<script src="/assets/js/main.54f950d1.js"></script>
</body>
</html>