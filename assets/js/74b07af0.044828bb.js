"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[23],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),u=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=u(a),d=r,h=m["".concat(s,".").concat(d)]||m[d]||c[d]||o;return a?n.createElement(h,i(i({ref:t},p),{},{components:a})):n.createElement(h,i({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=a[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},64:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>u});var n=a(7462),r=(a(7294),a(3905));const o={sidebar_position:1},i="Audio ~ HuggingFace Course",l={unversionedId:"notes/audio/audio-hf",id:"notes/audio/audio-hf",title:"Audio ~ HuggingFace Course",description:"Link to the course",source:"@site/docs/notes/audio/audio-hf.md",sourceDirName:"notes/audio",slug:"/notes/audio/audio-hf",permalink:"/docs/notes/audio/audio-hf",draft:!1,editUrl:"https://github.com/sam99dave/sam99dave.github.io/tree/main/docs/docs/notes/audio/audio-hf.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Audio",permalink:"/docs/category/audio"},next:{title:"Designing ML Systems",permalink:"/docs/category/designing-ml-systems"}},s={},u=[{value:"Audio Data Intro",id:"audio-data-intro",level:2},{value:"<em>Continuous to Digital Conversion</em>",id:"continuous-to-digital-conversion",level:3},{value:"Sampling &amp; Sampling Rate",id:"sampling--sampling-rate",level:2},{value:"Sampling Rate",id:"sampling-rate",level:3},{value:"Amplitude &amp; Bit depth",id:"amplitude--bit-depth",level:2},{value:"32-bit Audio?",id:"32-bit-audio",level:3},{value:"Real vs Digital Audio",id:"real-vs-digital-audio",level:2},{value:"Frequency Spectrum",id:"frequency-spectrum",level:2},{value:"Spectrogram",id:"spectrogram",level:2},{value:"Generation process",id:"generation-process",level:3},{value:"Spectrogram to Waveform",id:"spectrogram-to-waveform",level:3},{value:"Mel Spectrogram",id:"mel-spectrogram",level:2},{value:"Mel vs Standard Spectrogram",id:"mel-vs-standard-spectrogram",level:3},{value:"Generation Process",id:"generation-process-1",level:3},{value:"Log Mel Spectrogram",id:"log-mel-spectrogram",level:3}],p={toc:u},m="wrapper";function c(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"audio--huggingface-course"},"Audio ~ HuggingFace Course"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/learn/audio-course/chapter0/introduction#welcome-to-the-hugging-face-audio-course"},"Link to the course")),(0,r.kt)("h2",{id:"audio-data-intro"},"Audio Data Intro"),(0,r.kt)("p",null,"To be processed, stored, and transmitted by digital devices, the continuous sound wave needs to be converted into a series of discrete values, known as a ",(0,r.kt)("inlineCode",{parentName:"p"},"digital representation"),"."),(0,r.kt)("h3",{id:"continuous-to-digital-conversion"},(0,r.kt)("em",{parentName:"h3"},"Continuous to Digital Conversion")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The analog signal is first captured by a microphone, which converts the sound waves into an electrical signal."),(0,r.kt)("li",{parentName:"ul"},"The electrical signal is then digitized by an Analog-to-Digital Converter to get the digital representation through sampling.")),(0,r.kt)("h2",{id:"sampling--sampling-rate"},"Sampling & Sampling Rate"),(0,r.kt)("p",null,"The process of measuring values of a continuous signal at fixed timesteps is called as sampling."),(0,r.kt)("p",null,"The waveform obtained from this process contains finite number of signal values at uniform intervals therefore, it\u2019s called to be ",(0,r.kt)("inlineCode",{parentName:"p"},"discrete"),"."),(0,r.kt)("h3",{id:"sampling-rate"},"Sampling Rate"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Also called as sampling frequency"),(0,r.kt)("li",{parentName:"ul"},"It indicates the number of samples taken per second ( also represented as Hz ). e.g. 1000Hz means 1000 samples per second."),(0,r.kt)("li",{parentName:"ul"},"High resolution samples have sampling rate of 192kHz.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"A common sampling rate used in training speech models is 16kHz.")),(0,r.kt)("p",null,"E.g. Audible frequencies in human speech < 8kHz. Using sampling rate too small or too big won\u2019t benefit."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("em",{parentName:"p"},"Using higher sampling rate")),(0,r.kt)("blockquote",{parentName:"blockquote"},(0,r.kt)("p",{parentName:"blockquote"},"Doesn\u2019t capture more information. It just increases the computation cost.")),(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("em",{parentName:"p"},"Using lower sampling rate")),(0,r.kt)("blockquote",{parentName:"blockquote"},(0,r.kt)("p",{parentName:"blockquote"},"Results in information loss. e.g. muffled sound etc."))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Training & testing dataset care")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Its important to ensure that the audio samples that are being used in the datasets are having the same sampling rate."),(0,r.kt)("li",{parentName:"ul"},"In case of fine-tuning a pretrained model. Its crucial to validate that the custom audio samples have the same sampling rate as that of the data that the model was trained on."),(0,r.kt)("li",{parentName:"ul"},"Sampling rate represents the interval between the successive samples so change in it will an affect on the temporal resolution of the audio data."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Resampling")," is a process that is a part of preprocessing which makes the sampling rates to match.")),(0,r.kt)("h2",{id:"amplitude--bit-depth"},"Amplitude & Bit depth"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Sampling rate tells you how often the samples are taken, what exactly are the values in each sample?")),(0,r.kt)("p",null,"When there is a change in air pressure at a frequency that is audible to humans we hear a sound."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Amplitude")," - Sound pressure level at any given instant ( measured in dB )"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Bit Depth")," - Its the precision of describing this amplitude value."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"The higher the bit depth, the more faithfully the digital representation approximates the original continuous sound wave.")),(0,r.kt)("p",null,"Continuous to discrete conversion involves rounding off values which introduces some noise called as quantization noise. The higher the bit depth the smaller the quantization noise.\n",(0,r.kt)("strong",{parentName:"p"},"16-bit")," and ",(0,r.kt)("strong",{parentName:"p"},"24-bit")," are the most common audio bit depth."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"16-bit audio bit depth itself has noise which is not audible to humans. Therefore, using higher bit depths are generally not necessary.")),(0,r.kt)("h3",{id:"32-bit-audio"},"32-bit Audio?"),(0,r.kt)("p",null,"It stores samples as floating points which is great as models are trained on floating points whereas, 16-bit and 24-bit store as integers."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The precision of 32-bit is same as that of 24-bit ( bit depth )."),(0,r.kt)("li",{parentName:"ul"},"Floating point audio samples are expected to lie between ","[ -1.0, 1.0 ]")),(0,r.kt)("h2",{id:"real-vs-digital-audio"},"Real vs Digital Audio"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Real Audio")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"0 dB represents quietest possible sound humans can hear."),(0,r.kt)("li",{parentName:"ul"},"Louder sounds have larger values.")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Digital Audio")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"0 dB is the loudest possible amplitude while other amplitudes are negative."),(0,r.kt)("li",{parentName:"ul"},"Anything below -60 dB is generally inaudible unless the volume is cranked up.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Rule Of Thumb - Every -6 dB is halving of the amplitude.")),(0,r.kt)("h2",{id:"frequency-spectrum"},"Frequency Spectrum"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/learn/audio-course/chapter1/audio_data#the-frequency-spectrum"},(0,r.kt)("inlineCode",{parentName:"a"},"I'll recommend to view the section itself from HugingFace Audio Course"))),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"FFT")," - Fast Fourier Transform"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"DFT")," - Discrete Fourier Transform"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"In practice, people use the term FFT interchangeably with DFT, as the FFT or Fast Fourier Transform is the only efficient way to calculate the DFT on a computer.")),(0,r.kt)("h2",{id:"spectrogram"},"Spectrogram"),(0,r.kt)("h3",{id:"generation-process"},"Generation process"),(0,r.kt)("p",null,"Generate multiple DFTs, each covering only a small slice of time and stack the resulting spectra together to form a spectrogram ~\n",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/learn/audio-course/chapter1/audio_data#spectrogram"},"code snippet")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Spectrogram plots the frequency content of an audio signal as it changes over time. "),(0,r.kt)("li",{parentName:"ul"},"It shows time, frequency and amplitude in one graph."),(0,r.kt)("li",{parentName:"ul"},"The algorithm that performs this computation is the STFT or ",(0,r.kt)("inlineCode",{parentName:"li"},"Short Time Fourier Transform"))),(0,r.kt)("h3",{id:"spectrogram-to-waveform"},"Spectrogram to Waveform"),(0,r.kt)("p",null,"Method - ",(0,r.kt)("inlineCode",{parentName:"p"},"Inverse STFT")),(0,r.kt)("p",null,"This requires ",(0,r.kt)("strong",{parentName:"p"},"phase information")," in addition to ",(0,r.kt)("strong",{parentName:"p"},"amplitude information"),"."),(0,r.kt)("p",null,"If the spectrogram is generated using a ML model then its likely that we only have the amplitude info, as typically ML models only output amplitudes.\nWe can use a phase reconstruction algorithm such as classic ",(0,r.kt)("strong",{parentName:"p"},"Griffin-Lim algorithm")," or using a NN called ",(0,r.kt)("strong",{parentName:"p"},"vocoder")," to reconstruct a waveform from a spectrogram."),(0,r.kt)("h2",{id:"mel-spectrogram"},"Mel Spectrogram"),(0,r.kt)("p",null,"A variation of the spectrogram that is commonly used in speech processing and machine learning tasks."),(0,r.kt)("h3",{id:"mel-vs-standard-spectrogram"},"Mel vs Standard Spectrogram"),(0,r.kt)("p",null,"It is similar to a spectrogram in that it shows the frequency content of an audio signal over time, but on a different frequency axis."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Standard Spectrogram")," ~ Frequency axis is linear"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Mel")," ~ Non-linear frequency"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Why this matters you might ask?")),(0,r.kt)("p",null,"The human auditory system is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as frequency increases. Mel scale is a perceptual scale that approximates the non-linear frequency of the human ear."),(0,r.kt)("h3",{id:"generation-process-1"},"Generation Process"),(0,r.kt)("p",null,"Similar to standard spectrogram generation, only each and every spectra is sent through a set of filters called ",(0,r.kt)("strong",{parentName:"p"},"mel filterbank")," which transforms the frequencies to ",(0,r.kt)("strong",{parentName:"p"},"mel scale"),"."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},'Not all mel spectrograms are the same! There are two different mel scales in common use ("htk" and "slaney"), and instead of the power spectrogram the amplitude spectrogram may be used. The conversion to a log-mel spectrogram doesn\'t always compute true decibels but may simply take the log. Therefore, if a machine learning model expects a mel spectrogram as input, double check to make sure you\'re computing it the same way.')),(0,r.kt)("h3",{id:"log-mel-spectrogram"},"Log Mel Spectrogram"),(0,r.kt)("p",null,"Just as with a regular spectrogram, it\u2019s common practice to express the strength of the mel frequency components in decibels. This is commonly referred to as a\xa0log-mel spectrogram, because the conversion to decibels involves a logarithmic operation."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Mel to waveform conversion is difficult as compared to that from a standard one. ML models such as ",(0,r.kt)("strong",{parentName:"p"},"HiFiGAN vocoder")," are needed to produce a waveform from a mel spectrogram.")))}c.isMDXComponent=!0}}]);