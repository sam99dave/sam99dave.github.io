"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[849],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=u(n),g=i,m=p["".concat(s,".").concat(g)]||p[g]||d[g]||o;return n?a.createElement(m,r(r({ref:t},c),{},{components:n})):a.createElement(m,r({ref:t},c))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:i,r[1]=l;for(var u=2;u<o;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},4419:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>u});var a=n(7462),i=(n(7294),n(3905));const o={sidebar_position:2},r="Feature Engineering",l={unversionedId:"notes/designing-ml-systems/feature-engineering",id:"notes/designing-ml-systems/feature-engineering",title:"Feature Engineering",description:"A large part of many ML engineering and data science is to come up with new useful features.",source:"@site/docs/notes/designing-ml-systems/feature-engineering.md",sourceDirName:"notes/designing-ml-systems",slug:"/notes/designing-ml-systems/feature-engineering",permalink:"/docs/notes/designing-ml-systems/feature-engineering",draft:!1,editUrl:"https://github.com/sam99dave/sam99dave.github.io/tree/main/docs/docs/notes/designing-ml-systems/feature-engineering.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/docs/notes/designing-ml-systems/intro"},next:{title:"Purpose",permalink:"/docs/notes/intro"}},s={},u=[{value:"Common Feature Engineering Operations",id:"common-feature-engineering-operations",level:2},{value:"Handling Missing Values",id:"handling-missing-values",level:3},{value:"Deletion",id:"deletion",level:4},{value:"Imputation",id:"imputation",level:4},{value:"Scaling",id:"scaling",level:3},{value:"0, 1",id:"0-1",level:4},{value:"a, b",id:"a-b",level:4},{value:"Standardization (Normal Distribution)",id:"standardization-normal-distribution",level:4},{value:"Log Transformation",id:"log-transformation",level:4},{value:"Discretization",id:"discretization",level:3},{value:"Encoding Categorical Features",id:"encoding-categorical-features",level:3},{value:"Solution",id:"solution",level:4},{value:"Feature Crossing",id:"feature-crossing",level:3}],c={toc:u},p="wrapper";function d(e){let{components:t,...o}=e;return(0,i.kt)(p,(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"feature-engineering"},"Feature Engineering"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"A large part of many ML engineering and data science is to come up with new useful features.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Promise of Deep Learning")),(0,i.kt)("p",null,"Deep learning is also called as feature learning because these algorithms are great at automatically extracting and learning features. However, we are still far from the point where all features can be extracted automatically. Moreover, not all ML systems are deep learning based."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"n-gram feature extraction process",src:n(8886).Z,title:"n-gram feature extraction process",width:"2476",height:"1822"})),(0,i.kt)("h2",{id:"common-feature-engineering-operations"},"Common Feature Engineering Operations"),(0,i.kt)("h3",{id:"handling-missing-values"},"Handling Missing Values"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Not all types of missing values are equal. There is information in them being missing.")),(0,i.kt)("p",null,"There are in total 3 types of missing values:"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Missing not at random (MNAR)")),(0,i.kt)("p",null,"The reason why the value is missing is because of the the true value itself."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Income values are missing for reasons related to the value themselves (high income are always not disclosed)")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Missing at random (MAR)")),(0,i.kt)("p",null,"When the value is missing not due to the value itself but due to another observed variable."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"People of gender ",(0,i.kt)("inlineCode",{parentName:"li"},"A")," have not disclosed their age (They might not like to disclose their age). This is information.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Mising completely at random (MCAR)")),(0,i.kt)("p",null,"There is no pattern for the missing values."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"People just forget to fill values sometimes for no particular reasons, jobs .etc")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"This is considered to be very rare. There is usually some reasons for missing values.")),(0,i.kt)("h4",{id:"deletion"},"Deletion"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Column Deletion")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Row Deletion")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Consider the affect of removing all above mentioned types of missing values on the traning. Refer to examples in the book ~ pg 125")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Deleting data results in loss of information and can result in loss of significant information")),(0,i.kt)("h4",{id:"imputation"},"Imputation"),(0,i.kt)("p",null,"Filling default values, median, mode or mean."),(0,i.kt)("p",null,"Avoid adding random values or fixed defaults eg. 0 such values will have a negative affect on training and very difficult to debug."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"With imputation, you risk injecting your own bias into and adding noise to you data, or worse data leakage.")),(0,i.kt)("h3",{id:"scaling"},"Scaling"),(0,i.kt)("p",null,"Also known as feature scaling. Neglecting to do so can cause models to make gibberish predictions, especially with classical algorithms like ",(0,i.kt)("inlineCode",{parentName:"p"},"gradient boosting trees")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"logistic regression"),"."),(0,i.kt)("h4",{id:"0-1"},"[0, 1]"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"0-1 range",src:n(112).Z,title:"[0,1] range",width:"2592",height:"841"})),(0,i.kt)("h4",{id:"a-b"},"[a, b]"),(0,i.kt)("p",null,"sdfasdfasd"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"a-b range",src:n(6756).Z,title:"[a,b] range",width:"2592",height:"967"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Scaling to a arbitrary range works well if you don't want to make an assumptions about the variables. ")),(0,i.kt)("h4",{id:"standardization-normal-distribution"},"Standardization (Normal Distribution)"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Continuing to the above highlight, if you think that your variables follow a normal distribution, it might be helpful to normalize them ~ zero mean & unit variance.")),(0,i.kt)("p",null,"This process is called as ",(0,i.kt)("inlineCode",{parentName:"p"},"Standardization")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Standardization",src:n(4883).Z,title:"Standardization",width:"1362",height:"904"})),(0,i.kt)("h4",{id:"log-transformation"},"Log Transformation"),(0,i.kt)("p",null,"ML models tend to struggle with features that follow ",(0,i.kt)("inlineCode",{parentName:"p"},"skewed distribution"),"."),(0,i.kt)("p",null,"Log transformation is a commonly used technique to mitigate this skewness."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"This applies log function to the feature."),(0,i.kt)("li",{parentName:"ul"},"In some cases it might not be as helpful as expected.")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Be careful of the data analysis if performed on the log transformed data instead of the original one ~ Go with the original data for any analysis")),(0,i.kt)("h3",{id:"discretization"},"Discretization"),(0,i.kt)("p",null,"The book has covered this for completeness. According to the author, ",(0,i.kt)("inlineCode",{parentName:"p"},"Chip Huyen")," discretization is found to be rarely helpful."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"It is the process of turning continuous features into a discrete feature. It is also called as quantization or binning")),(0,i.kt)("p",null,"It introduces discontinuities at the boudaries."),(0,i.kt)("p",null,"E.g. Earning is quantized as 0-500, 500-1000, 1000-2000 etc. Here 499 is close to the 2nd class but it is treated as a completely different class."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Choosing the boundaries can be difficult."),(0,i.kt)("li",{parentName:"ul"},"One can plot ",(0,i.kt)("inlineCode",{parentName:"li"},"histograms")," of the values and choose the boundaries that makes sense.")),(0,i.kt)("h3",{id:"encoding-categorical-features"},"Encoding Categorical Features"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Assumption")," ~ Categories are always static, it doesn't change over time. This is not ",(0,i.kt)("strong",{parentName:"p"},"true"),". In production, categories change."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Refer to the book for nice Amazon example ~ pg 130"),". Overview, new products are added daily changing & updating the categories."),(0,i.kt)("h4",{id:"solution"},"Solution"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Hashing trick")," is one solution to this problem ( ",(0,i.kt)("inlineCode",{parentName:"p"},"Vowpal Wabbit")," )."),(0,i.kt)("p",null,"There always exist the problem of ",(0,i.kt)("strong",{parentName:"p"},"collision"),". Hash space can be increased by using a larger number of bits."),(0,i.kt)("p",null,"However, with many hash functions, the collisions are random, there is no inherent pattern to it. The impact of colliding hashed features is, fortunately, not that bad."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"In a research done by Booking.com, even for 50% colliding features, the performance loss is less than 0.5%, check the below figure.")),(0,i.kt)("p",null,"One can also choose a hash function with properties one wants, such as ",(0,i.kt)("inlineCode",{parentName:"p"},"locality-sensitive hashing function")," where similar categories (similar names, etc) are hashed into values ",(0,i.kt)("em",{parentName:"p"},"close to each other"),"."),(0,i.kt)("h3",{id:"feature-crossing"},"Feature Crossing"),(0,i.kt)("p",null,"This technique is useful to model the non-linear relationships between features. This is essential for models that can't learn or are bad at learning non-linear relationships, such as ",(0,i.kt)("inlineCode",{parentName:"p"},"linear regression"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"logistic regression")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"tree based models"),". "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"This is less important in ",(0,i.kt)("em",{parentName:"li"},"Neural Networks"),".")))}d.isMDXComponent=!0},112:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/0-1-c302803286d05ed33b2734f5237d68c5.jpg"},6756:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/a-b-bcff95aafd8fa2cb3aef4c6380a49816.jpg"},8886:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/n-gram feature extraction process -8c89167d6b1ed4117c73708eba07bc31.jpg"},4883:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/standardization-9360c6c968b234e8cb2d5e2fa4f5db24.jpg"}}]);