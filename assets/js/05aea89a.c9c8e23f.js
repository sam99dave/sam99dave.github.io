"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[9369],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>u});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=p(a),d=r,u=c["".concat(l,".").concat(d)]||c[d]||h[d]||i;return a?n.createElement(u,o(o({ref:t},m),{},{components:a})):n.createElement(u,o({ref:t},m))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},5328:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const i={slug:"LoRA ~ Low-Rank Adaptation",title:"LoRA ~ Low-Rank Adaptation",authors:{name:"Samuel Davis",title:"Docusaurus Core Team",url:"https://github.com/wgao19",image_url:"https://github.com/wgao19.png"},tags:["LoRA","Large Language Models"]},o=void 0,s={permalink:"/blog/LoRA ~ Low-Rank Adaptation",editUrl:"https://github.com/sam99dave/sam99dave.github.io/tree/main/blog/blog/LoRA/LoRA.md",source:"@site/blog/LoRA/LoRA.md",title:"LoRA ~ Low-Rank Adaptation",description:"Fine Tuning models such as GPT-3 ( 175B ) from scratch requires lot of compute which is very costly. Low Rank Adaptation ( LoRA ) freezes the pretrained model weights & injects trainable rank decomposition matrices. These matrices are injected into each layer of the Transformer architecture. This reduces the number of trainable parameters for downstream tasks.",date:"2023-12-23T15:12:38.000Z",formattedDate:"December 23, 2023",tags:[{label:"LoRA",permalink:"/blog/tags/lo-ra"},{label:"Large Language Models",permalink:"/blog/tags/large-language-models"}],readingTime:7.655,hasTruncateMarker:!1,authors:[{name:"Samuel Davis",title:"Docusaurus Core Team",url:"https://github.com/wgao19",image_url:"https://github.com/wgao19.png",imageURL:"https://github.com/wgao19.png"}],frontMatter:{slug:"LoRA ~ Low-Rank Adaptation",title:"LoRA ~ Low-Rank Adaptation",authors:{name:"Samuel Davis",title:"Docusaurus Core Team",url:"https://github.com/wgao19",image_url:"https://github.com/wgao19.png",imageURL:"https://github.com/wgao19.png"},tags:["LoRA","Large Language Models"]},nextItem:{title:"Welcome",permalink:"/blog/welcome"}},l={authorsImageUrls:[void 0]},p=[],m={toc:p},c="wrapper";function h(e){let{components:t,...i}=e;return(0,r.kt)(c,(0,n.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Fine Tuning models such as GPT-3 ( 175B ) from scratch requires lot of compute which is very costly. ",(0,r.kt)("strong",{parentName:"p"},"Lo"),"w ",(0,r.kt)("strong",{parentName:"p"},"R"),"ank ",(0,r.kt)("strong",{parentName:"p"},"A"),"daptation ( ",(0,r.kt)("strong",{parentName:"p"},"LoRA")," ) freezes the pretrained model weights & injects trainable rank decomposition matrices. These matrices are injected into each layer of the Transformer architecture. This reduces the number of trainable parameters for downstream tasks."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times.")),(0,r.kt)("p",null,"LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers\u2019 change during\nadaptation instead, while keeping the pre-trained weights frozen"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Advantages")),(0,r.kt)("p",null,"\u2192 A pre-trained model can be shared to build LoRA modules for different tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices ",(0,r.kt)("em",{parentName:"p"},"A")," and ",(0,r.kt)("em",{parentName:"p"},"B,")," reducing the storage requirement and task-switching overhead significantly."),(0,r.kt)("p",null,"\u2192 LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices."),(0,r.kt)("p",null,"\u2192 The simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, ",(0,r.kt)("em",{parentName:"p"},"introducing no inference latency")," compared to a fully fine-tuned model, by construction."),(0,r.kt)("p",null,"\u2192 LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning."),(0,r.kt)("p",null,"Read From ",(0,r.kt)("a",{parentName:"p",href:"https://www.ml6.eu/blogpost/low-rank-adaptation-a-technical-deep-dive#:~:text=The%20main%20idea%20behind%20LoRA,a%20low%2Drank%20matrix%20i.e."},"here")," and explain details of LoRA"),(0,r.kt)("hr",null),(0,r.kt)("p",null,"During full fine-tuning, the model is initialized to pre-trained weights \u03a60 and updated to \u03a60 + \u2206\u03a6 by repeatedly following the gradient to maximize the conditional language modeling objective:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(3886).Z,width:"388",height:"92"})),(0,r.kt)("p",null,"One of the main drawbacks for full fine-tuning is that for ",(0,r.kt)("em",{parentName:"p"},"each")," downstream task, we learn a ",(0,r.kt)("em",{parentName:"p"},"different")," set of parameters \u2206\u03a6 whose dimension ",(0,r.kt)("em",{parentName:"p"},"|"),"\u2206\u03a6",(0,r.kt)("em",{parentName:"p"},"|")," equals ",(0,r.kt)("em",{parentName:"p"},"|"),"\u03a60",(0,r.kt)("em",{parentName:"p"},"|"),"."),(0,r.kt)("p",null,"If a model has weight matrices of size WxH then the delta will also be of the same dimension. Therefore, while finetuning if there is a model with dimension 175B then the delta will also require a storage space to save these gradients of size 175B before updating the weights."),(0,r.kt)("p",null,"Moreover, storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible."),(0,r.kt)("p",null,"In this paper, we adopt a more parameter-efficient approach, where the task-specific parameter increment \u2206\u03a6 = \u2206\u03a6(\u0398) is further ",(0,r.kt)("strong",{parentName:"p"},"encoded by a much smaller-sized set of parameters")," \u0398 with ",(0,r.kt)("em",{parentName:"p"},"|"),"\u0398",(0,r.kt)("em",{parentName:"p"},"| << |"),"\u03a60",(0,r.kt)("em",{parentName:"p"},"|"),". "),(0,r.kt)("p",null,"The task of finding \u2206\u03a6 thus becomes optimizing over \u0398:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(1111).Z,width:"463",height:"98"})),(0,r.kt)("p",null,"The authors propose to use a low-rank representation to encode \u2206\u03a6 that is both ",(0,r.kt)("strong",{parentName:"p"},"compute- and memory-efficient"),". "),(0,r.kt)("p",null,"When the pre-trained model is GPT-3 175B, the number of train able parameters ",(0,r.kt)("em",{parentName:"p"},"|"),"\u0398",(0,r.kt)("em",{parentName:"p"},"|")," can be as small as 0",(0,r.kt)("em",{parentName:"p"},"."),"01% of ",(0,r.kt)("em",{parentName:"p"},"|"),"\u03a60",(0,r.kt)("em",{parentName:"p"},"|")," ( 175B \u2192 17.5M )."),(0,r.kt)("p",null,"Aren\u2019t Existing Solutions Good Enough?"),(0,r.kt)("p",null,"This issue has been there since ",(0,r.kt)("em",{parentName:"p"},"Transfer Learning")," "),(0,r.kt)("p",null,"Many techniques have been developed trying to tackle this. From Language Modelling POV, there are two strategies:"),(0,r.kt)("p",null,"\u2192 Adding Adapter layers"),(0,r.kt)("p",null,"\u2192 Optimizing some forms of the input layer activation"),(0,r.kt)("p",null,"However, there are limitations to them"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Adding Adapter layers")," \u2192 Introduces Inference Latency"),(0,r.kt)("p",null,"\u2192 One can think of reducing the latency by either pruning the layers or by exploiting multi-task settings but there is no direct way to bypass the extra compute that the adapters add in. It might seem that its fine as the adapters have few parameters (almost < 1% of the model) due to bottleneck dimensions, which limits the FLOPs they can add. Large Neural Networks rely on GPU parallelism to keep the latency low, ",(0,r.kt)("strong",{parentName:"p"},"adapter layers have to be processed sequentially"),". On performing inference using GPT-2 medium on a single GPU (scenario without model parallelism),"),(0,r.kt)("p",null,"we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(2171).Z,width:"1161",height:"438"})),(0,r.kt)("hr",null),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"LoRA")),(0,r.kt)("p",null,"\u2192 LOW-RANK-PARAMETRIZED UPDATE MATRICES"),(0,r.kt)("p",null,"A neural network contains many dense layers which perform matrix multiplication. The weight\nmatrices in these layers typically have ",(0,r.kt)("strong",{parentName:"p"},"full-rank.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Lets check out Ranks first")),(0,r.kt)("p",null,"Matrix Rank"),(0,r.kt)("p",null,"The\xa0",(0,r.kt)("strong",{parentName:"p"},"rank of a matrix"),"\xa0is the dimension of the vector space generated by its columns, which is given by the\xa0",(0,r.kt)("strong",{parentName:"p"},"number of\xa0linearly independent\xa0columns (or rows) in a given matrix."),"\xa0"),(0,r.kt)("p",null,"It can be proven that the number of independent columns (known as\xa0",(0,r.kt)("em",{parentName:"p"},"column rank)"),"\xa0is always equal to the number of independent rows (called\xa0",(0,r.kt)("em",{parentName:"p"},"row rank)"),". Hence, for a matrix\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"A")),"\xa0with\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"m")),"\xa0rows and\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"n")),"\xa0columns (represented as\xa0",(0,r.kt)("strong",{parentName:"p"},"*A\u2098\u2099"),")*,"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(6142).Z,width:"607",height:"112"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Linear Dependence & Independence")),(0,r.kt)("p",null,"In the theory of\xa0",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Vector_space"},"vector spaces"),", a\xa0",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Set_(mathematics)"},"set"),"\xa0of\xa0",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Vector_(mathematics)"},"vectors"),"\xa0is said to be\xa0",(0,r.kt)("strong",{parentName:"p"},"linearly independent"),"\xa0if there exists no nontrivial\xa0",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Linear_combination"},"linear combination"),"\xa0of the vectors that equals the zero vector."),(0,r.kt)("p",null,"\u2192 Linear Dependent"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(4600).Z,width:"913",height:"196"})),(0,r.kt)("p",null,"\u2192 Linear Independent"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(1588).Z,width:"1001",height:"73"})),(0,r.kt)("p",null,"Can only be satisfied if a(i) = 0 for i = 1,\u2026,n"),(0,r.kt)("p",null,"Based on these Ranks matrices can be classified into 2 types:"),(0,r.kt)("p",null,"\u2192 Full Rank"),(0,r.kt)("p",null,"A matrix\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"A\u2098\u2099")),"\xa0is called a\xa0",(0,r.kt)("strong",{parentName:"p"},"full-rank matrix"),"\xa0if\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"rank(A) ="),"\xa0",(0,r.kt)("em",{parentName:"strong"},"min(m, n)"),"."),"\xa0The matrix shown below is an example of a full rank matrix."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(3793).Z,width:"672",height:"257"})),(0,r.kt)("p",null,"\u2192 Rank Deficient"),(0,r.kt)("p",null,"The opposite of a full rank matrix is\xa0",(0,r.kt)("strong",{parentName:"p"},"rank deficient"),"\xa0i.e.\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"rank(A)"),"\xa0<\xa0",(0,r.kt)("em",{parentName:"strong"},"min(m, n)")),". The rank-deficient matrix shown below has a rank of\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"1")),", as the columns (or rows) of the matrix are not linearly independent of one another."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(7349).Z,width:"651",height:"177"})),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"L",(0,r.kt)("strong",{parentName:"em"},"ow-Rank Matrix"),": A rank-deficient matrix\xa0",(0,r.kt)("strong",{parentName:"em"},"A\u2098\u2099"),"\xa0is called a low-rank matrix if its rank is significantly lower (no fixed threshold) than the minimum number of rows and columns. Mathematically,\xa0",(0,r.kt)("strong",{parentName:"em"},"rank(A) << min(m, n)"),".")),(0,r.kt)("p",null,"Rank Decomposition"),(0,r.kt)("p",null,"Rank decomposition or factorization of a matrix\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"A\u2098\u2099")),"\xa0is the factorization of\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"A")),"\xa0of the form\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"A")),"\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"= C\u2098\u1d63F\u1d63\u2099")),"\xa0where\xa0",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"rank(A) ="),"\xa0",(0,r.kt)("em",{parentName:"strong"},"r")),". It can be proven that every (finite) matrix has a rank decomposition. "),(0,r.kt)("p",null,"Techniques like\xa0SVD\xa0(Singular Value Decomposition) can be used to construct such a decomposition."),(0,r.kt)("p",null,"When adapting to a task the pretrained model weights have \u201clow intrinsic dimensions\u201d. When adapting to a specific task (fine-tuning) we just randomly project it to a smaller subspace still it learns efficiently. Hence it can be hypothesized that the change in weights (update) also has a \u201clow intrinsic dimensions/rank\u201d. "),(0,r.kt)("p",null,"Therefore, we can use rank decomposition for the change in weights."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(9426).Z,width:"1131",height:"212"})),(0,r.kt)("p",null,"E.g."),(0,r.kt)("p",null,"If d = 100 & k = 100"),(0,r.kt)("p",null,"then d * k = 10000"),(0,r.kt)("p",null,"Now, if r ~ 5"),(0,r.kt)("p",null,"then ( d ",(0,r.kt)("em",{parentName:"p"}," r ) + ( r ")," k ) = ( 500 ) + ( 500 ) = 1000"),(0,r.kt)("p",null,"Its clear that how this reduces the storage requirements."),(0,r.kt)("p",null,"Initial Initialization,"),(0,r.kt)("p",null,"A \u2192 Random Gaussian initialization"),(0,r.kt)("p",null,"B \u2192 0"),(0,r.kt)("p",null,"Therefore, initially AB \u2192 0. Moreover, it also scaled by (",(0,r.kt)("em",{parentName:"p"},"alpha / rank"),")"),(0,r.kt)("p",null,"Where ",(0,r.kt)("em",{parentName:"p"},"alpha")," is a constant (this is set to the first selected rank). This helps to reduce the need to retune hyperparameters when varying ",(0,r.kt)("em",{parentName:"p"},"rank")),(0,r.kt)("p",null,"Latency"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(8789).Z,width:"1135",height:"141"})),(0,r.kt)("p",null,"Instead of saving 2 entire different models we use a shared pretrained model with task related adapters."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Practical Benefits & Limitations")),(0,r.kt)("p",null,"\u2192 The most significant benefit comes from the reduction in memory and storage usage as we do not need to store the optimizer states for the frozen parameters. This saves a lot of VRAM."),(0,r.kt)("p",null,"\u2192 The size of the checkpoint also significantly reduces. "),(0,r.kt)("p",null,"\u2192 Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters."),(0,r.kt)("p",null,"For GPT 3 175B, a speedup of 25% was observed for training compared to full fine-tuning."),(0,r.kt)("p",null,"Limitations"),(0,r.kt)("p",null,"\u2192 It is not straightforward to batch inputs to different tasks with different ",(0,r.kt)("em",{parentName:"p"},"A")," and ",(0,r.kt)("em",{parentName:"p"},"B")," in a single forward pass."),(0,r.kt)("hr",null),(0,r.kt)("p",null,"Storage Reduction"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(3930).Z,width:"1116",height:"132"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"UNDERSTANDING THE LOW-RANK UPDATES")),(0,r.kt)("p",null,"WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(2435).Z,width:"1081",height:"222"})),(0,r.kt)("p",null,"This is for GPT 3 175B, the budget for trainable parameters is set to 18M which ~ 35MB in FP16 storage."),(0,r.kt)("p",null,"The above figure shows us that:"),(0,r.kt)("p",null,"\u2192 Wq, Wv ~ Gives the best performance."),(0,r.kt)("p",null,"The figure suggests that even a rank of four captures enough information in \u2206",(0,r.kt)("em",{parentName:"p"},"W")," such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank."),(0,r.kt)("p",null,"WHAT IS THE OPTIMAL RANK ",(0,r.kt)("em",{parentName:"p"},"r")," FOR LORA?"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(7971).Z,width:"993",height:"278"})),(0,r.kt)("p",null,"LoRA already performs competitively with a very small ",(0,r.kt)("em",{parentName:"p"},"r")," (more so for ",(0,r.kt)("em",{parentName:"p"},"{Wq, Wv}")," than just ",(0,r.kt)("em",{parentName:"p"},"Wq"),")"),(0,r.kt)("p",null,"This suggests the update matrix \u2206",(0,r.kt)("em",{parentName:"p"},"W")," could have a very small ",(0,r.kt)("strong",{parentName:"p"},"intrinsic rank")),(0,r.kt)("p",null,"To further support this finding, the authors checked the overlap of the subspaces learned by different choices of ",(0,r.kt)("em",{parentName:"p"},"r")," and by different random seeds."),(0,r.kt)("p",null,"The authors argue that increasing ",(0,r.kt)("em",{parentName:"p"},"r")," does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient."),(0,r.kt)("p",null,"HOW DOES THE ADAPTATION MATRIX \u2206",(0,r.kt)("em",{parentName:"p"},"W")," COMPARE TO ",(0,r.kt)("em",{parentName:"p"},"W")," ?"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(6395).Z,width:"1143",height:"502"})),(0,r.kt)("p",null,"Note: There are detailed figures regarding these studies in the Appendix section of the paper"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Untitled",src:a(6193).Z,width:"1127",height:"302"})),(0,r.kt)("p",null,"Conclusion from the above figure:"),(0,r.kt)("p",null,"\u2192 \u2206",(0,r.kt)("em",{parentName:"p"},"W")," has a stronger correlation with ",(0,r.kt)("em",{parentName:"p"},"W")," compared to a random matrix, indicating that \u2206",(0,r.kt)("em",{parentName:"p"},"W")," amplifies some features that are already in ",(0,r.kt)("em",{parentName:"p"},"W"),"."),(0,r.kt)("p",null,"\u2192 \u2206",(0,r.kt)("em",{parentName:"p"},"W ~ 0.32 | W ~ 21.67")),(0,r.kt)("p",null,"\u2192 Random ~ 0.02 | ",(0,r.kt)("em",{parentName:"p"},"W ~ 21.67")),(0,r.kt)("p",null,"\u2192 Instead of repeating the top singular directions of ",(0,r.kt)("em",{parentName:"p"},"W"),", \u2206",(0,r.kt)("em",{parentName:"p"},"W")," only ",(0,r.kt)("em",{parentName:"p"},"amplifies directions that are not emphasized in W.")),(0,r.kt)("p",null,"\u2192 The amplification factor is rather huge: "),(0,r.kt)("p",null,"\u2192 21",(0,r.kt)("em",{parentName:"p"},"."),"5 ",(0,r.kt)("em",{parentName:"p"},"\u2248")," 6",(0,r.kt)("em",{parentName:"p"},"."),"91",(0,r.kt)("em",{parentName:"p"},"/"),"0",(0,r.kt)("em",{parentName:"p"},"."),"32 for ",(0,r.kt)("em",{parentName:"p"},"r")," = 4 when compared to ",(0,r.kt)("em",{parentName:"p"},"r")," = 64"),(0,r.kt)("p",null,"This suggests that the low-rank adaptation matrix potentially ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model")),"."))}h.isMDXComponent=!0},8789:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Latency-be5fccd449900d3bc00aae36854c0c82.png"},4600:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Linear Dependent vectors-d64cfe228dee311869daa438b93d4227.png"},1588:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/Linear Independent Vectors-a197ba7ebeed82b34eb1b2508b990a24.png"},2171:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/adapter L-H issues-05e833c3e52a0267f59fc81206ce2691.png"},6193:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/delta W and W - part 2-37a9a50b7d9ea6b0a50a1debdc9b0608.png"},6395:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/delta W vs W - part 1-7e5f3cb2d777cd106324a96701166e75.png"},3886:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/fine tuning-63769c81ffd6d5e3ca6b6b5e8bfe49e1.png"},3793:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/full rank-27ed691f14c1b0c85412a9cb348e6698.png"},2435:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/identifying weights to use LoRA-8292733a8ae00ca618cc2ee28232b5e3.png"},9426:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/lora-rankdecomposition-c298c722f64ada572e6ff87c1c35a53e.png"},7971:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/optimal rank-dfb758f654baa4c39e00e8a65ced06cb.png"},7349:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/rank deficient-15cc679aab22ccd638350c6bdfe0f776.png"},6142:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/rank matrix-1c69f56f2fe82c7c11ecfe598fce83fa.png"},3930:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/storage reduction-ef9f31d51f8c68962bba39a95cdb096c.png"},1111:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/updated-eq-92fdb8e2cbc5898d1f344279b09e0ca9.png"}}]);